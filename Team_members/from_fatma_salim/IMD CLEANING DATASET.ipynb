{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bee9f9cc-3fc6-4491-a6db-55ffee210613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'aclImdb', 'aclImdb_v1 (1).tar.gz', 'baboon.png', 'Binary Classification with Both Algorithms.ipynb', 'Color Enhancement.ipynb', 'data', 'Data Dictionary - carprices.xlsx', 'diabetes.csv', 'draft.ipynb', 'Fish Weight Modeling for Market Insights.ipynb', 'Fish.csv', 'flat_unclean_data.ipynb', 'goldhill.bmp', 'Image Processing Basics (Grayscale Conversion and Filtering).ipynb', 'IMAGE processing.ipynb', 'Image Sharpening for Medical Imaging.ipynb', 'Image Transformation on baboon.png.ipynb', 'IMAGE.jpg', 'image3.jpg', 'IMD.ipynb', 'IMDB_cleaned_train.csv', 'Introduction_to_Histogram.ipynb', 'Iris Multi-class Classification with K-NN.ipynb', 'Keyword-Based News Classification Model.ipynb', 'laptop unclean data.ipynb', 'lenna.png', 'News Search Engine.ipynb', 'News_Category_Dataset_v3.json', 'news_index.csv', 'news_tfidf_matrix.npz', 'news_tfidf_vectorizer.joblib', 'numpy_task.ipynb', 'python dubizzle_scraper.ipynb', 'Regression Analysis.ipynb', 'Sales Data Simulation and Analysis.ipynb', 'Sales Performance Analysis with Walmart Data.ipynb', 'sharpened_image.jpg', 'smart_building.ipynb', 'smart_building_data.csv', 'Student Performance Regression.ipynb', 'Student_Performance.csv', 'surat_uncleaned.csv', 'Time Series Analysis of Daily Electricity Consumption.ipynb', 'turtle.jpg', 'Turtles All the Way web scraping.ipynb', 'turtles_scraped.csv', 'turtles_scraped_cleaned.csv', 'Untitled.ipynb', 'Untitled1.ipynb', 'Web Scraping + Image Processing.ipynb', 'x-rays.png', 'zeeland.jpg']\n",
      "['imdb.vocab', 'imdbEr.txt', 'README', 'test', 'train']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir(\".\"))          # should show \"aclImdb\"\n",
    "print(os.listdir(\"aclImdb\"))    # should show train, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e2a3113-4419-4241-a3d7-e939ec9d90c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bbuser\\AppData\\Local\\Temp\\ipykernel_8340\\3470322777.py:51: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  text = BeautifulSoup(text, \"html.parser\").get_text(separator=\" \")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review  \\\n",
      "0  In Panic In The Streets Richard Widmark plays ...   \n",
      "1  If you ask me the first one was really better ...   \n",
      "2  I am a big fan a Faerie Tale Theatre and I've ...   \n",
      "3  I just finished reading a book about Dillinger...   \n",
      "4  Greg Davis and Bryan Daly take some crazed sta...   \n",
      "\n",
      "                                      cleaned_review  \n",
      "0  panic street richard widmark play navy doctor ...  \n",
      "1  ask first one really better one look sarah rea...  \n",
      "2  big fan faerie tale theatre seen one best funn...  \n",
      "3  finished reading book dillinger movie horribly...  \n",
      "4  greg davis bryan daly take crazed statement te...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bbuser\\AppData\\Local\\Temp\\ipykernel_8340\\3470322777.py:51: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  text = BeautifulSoup(text, \"html.parser\").get_text(separator=\" \")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cleaned datasets saved: IMDB_cleaned_train.csv & IMDB_cleaned_test.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "for res in [\"stopwords\", \"punkt\", \"punkt_tab\", \"wordnet\"]:\n",
    "    nltk.download(res, quiet=True)\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def safe_extract(tar, path=\".\"):\n",
    "    \"\"\"Safely extract tar.gz to avoid path traversal attacks\"\"\"\n",
    "    for member in tar.getmembers():\n",
    "        member_path = os.path.join(path, member.name)\n",
    "        if not os.path.commonprefix([os.path.abspath(path), os.path.abspath(member_path)]) == os.path.abspath(path):\n",
    "            raise Exception(\"Attempted Path Traversal in Tar File\")\n",
    "    tar.extractall(path)\n",
    "\n",
    "# Extract dataset if not already extracted\n",
    "tar_file = \"aclImdb_v1.tar.gz\"\n",
    "if os.path.exists(tar_file) and not os.path.exists(\"aclImdb\"):\n",
    "    print(\"Extracting dataset...\")\n",
    "    with tarfile.open(tar_file, \"r:gz\") as tar:\n",
    "        safe_extract(tar, \".\")\n",
    "    print(\"Extraction completed!\")\n",
    "\n",
    "dataset_path = \"aclImdb\"\n",
    "\n",
    "\n",
    "# Cleaning Function\n",
    "def clean_review(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove HTML tags safely\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text(separator=\" \")\n",
    "    \n",
    "    # Remove URLs, emails\n",
    "    text = re.sub(r'http\\S+|www.\\S+|[\\w\\.-]+@[\\w\\.-]+', '', text)\n",
    "    \n",
    "    # Remove punctuation, numbers, emojis\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords and short tokens\n",
    "    tokens = [t for t in tokens if t not in stop_words and len(t) > 2]\n",
    "    \n",
    "    # Lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    \n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Load Dataset\n",
    "\n",
    "def load_imdb_dataset(path, subset=\"train\", sample_size=None):\n",
    "    data, labels = [], []\n",
    "    for label in [\"pos\", \"neg\"]:\n",
    "        folder = os.path.join(path, subset, label)\n",
    "        for fname in os.listdir(folder):\n",
    "            file_path = os.path.join(folder, fname)\n",
    "            if os.path.isfile(file_path):\n",
    "                with open(file_path, encoding=\"utf-8\") as f:\n",
    "                    review = f.read()\n",
    "                    data.append(review)\n",
    "                    labels.append(label)\n",
    "    \n",
    "    df = pd.DataFrame({\"review\": data, \"sentiment\": labels})\n",
    "    \n",
    "    if sample_size:\n",
    "        df = df.sample(sample_size, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "df_train = load_imdb_dataset(dataset_path, subset=\"train\", sample_size=10000)\n",
    "df_train[\"cleaned_review\"] = df_train[\"review\"].apply(clean_review)\n",
    "\n",
    "\n",
    "print(df_train[[\"review\", \"cleaned_review\"]].head(5))\n",
    "\n",
    "# Save cleaned train set\n",
    "df_train.to_csv(\"IMDB_cleaned_train.csv\", index=False)\n",
    "\n",
    "\n",
    "df_test = load_imdb_dataset(dataset_path, subset=\"test\")\n",
    "df_test[\"cleaned_review\"] = df_test[\"review\"].apply(clean_review)\n",
    "df_test.to_csv(\"IMDB_cleaned_test.csv\", index=False)\n",
    "\n",
    "print(\"✅ Cleaned datasets saved: IMDB_cleaned_train.csv & IMDB_cleaned_test.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
