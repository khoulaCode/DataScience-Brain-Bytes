{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f66edc5a-0ff4-4fe7-a18d-327585fb495c",
   "metadata": {},
   "source": [
    "<!-- loaded the Kaggle News Category Dataset from your local path:\n",
    "C:\\Users\\bbuser\\Desktop\\News_Category_Dataset_v3.json\n",
    "\n",
    "The dataset is in JSON Lines format (.json where each line = one record), so we used:\n",
    "\n",
    "pd.read_json(DATA_PATH, lines=True) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f800584f-1139-4642-a411-f33f062f398c",
   "metadata": {},
   "source": [
    "##### 1) Loading the dataset\n",
    "\n",
    "##### I used the Kaggle News Category Dataset from my local machine (News_Category_Dataset_v3.json).\n",
    "\n",
    "##### Since the file is in JSON-Lines format, I loaded it with the correct setting to read each line as a separate record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c91f70c-a257-4bb3-b91e-547032bd4103",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "DATA_PATH = r\"C:\\Users\\bbuser\\Desktop\\News_Category_Dataset_v3.json\"\n",
    "OUTPUT_DIR = r\"C:\\Users\\bbuser\\Desktop\\news_keyword_baseline\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "TARGET_CATEGORIES = [\"POLITICS\", \"TRAVEL\", \"SPORTS\", \"HOME & LIVING\"]\n",
    "SAMPLES_PER_CLASS = 1000\n",
    "TEST_SIZE = 0.20\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51d5c66e-930e-4f12-bf3c-6d124f360345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>headline</th>\n",
       "      <th>category</th>\n",
       "      <th>short_description</th>\n",
       "      <th>authors</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.huffpost.com/entry/covid-boosters-...</td>\n",
       "      <td>Over 4 Million Americans Roll Up Sleeves For O...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>Health experts said it is too early to predict...</td>\n",
       "      <td>Carla K. Johnson, AP</td>\n",
       "      <td>2022-09-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.huffpost.com/entry/american-airlin...</td>\n",
       "      <td>American Airlines Flyer Charged, Banned For Li...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>He was subdued by passengers and crew when he ...</td>\n",
       "      <td>Mary Papenfuss</td>\n",
       "      <td>2022-09-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.huffpost.com/entry/funniest-tweets...</td>\n",
       "      <td>23 Of The Funniest Tweets About Cats And Dogs ...</td>\n",
       "      <td>COMEDY</td>\n",
       "      <td>\"Until you have a dog you don't understand wha...</td>\n",
       "      <td>Elyse Wanshel</td>\n",
       "      <td>2022-09-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.huffpost.com/entry/funniest-parent...</td>\n",
       "      <td>The Funniest Tweets From Parents This Week (Se...</td>\n",
       "      <td>PARENTING</td>\n",
       "      <td>\"Accidentally put grown-up toothpaste on my to...</td>\n",
       "      <td>Caroline Bologna</td>\n",
       "      <td>2022-09-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.huffpost.com/entry/amy-cooper-lose...</td>\n",
       "      <td>Woman Who Called Cops On Black Bird-Watcher Lo...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>Amy Cooper accused investment firm Franklin Te...</td>\n",
       "      <td>Nina Golgowski</td>\n",
       "      <td>2022-09-22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                link  \\\n",
       "0  https://www.huffpost.com/entry/covid-boosters-...   \n",
       "1  https://www.huffpost.com/entry/american-airlin...   \n",
       "2  https://www.huffpost.com/entry/funniest-tweets...   \n",
       "3  https://www.huffpost.com/entry/funniest-parent...   \n",
       "4  https://www.huffpost.com/entry/amy-cooper-lose...   \n",
       "\n",
       "                                            headline   category  \\\n",
       "0  Over 4 Million Americans Roll Up Sleeves For O...  U.S. NEWS   \n",
       "1  American Airlines Flyer Charged, Banned For Li...  U.S. NEWS   \n",
       "2  23 Of The Funniest Tweets About Cats And Dogs ...     COMEDY   \n",
       "3  The Funniest Tweets From Parents This Week (Se...  PARENTING   \n",
       "4  Woman Who Called Cops On Black Bird-Watcher Lo...  U.S. NEWS   \n",
       "\n",
       "                                   short_description               authors  \\\n",
       "0  Health experts said it is too early to predict...  Carla K. Johnson, AP   \n",
       "1  He was subdued by passengers and crew when he ...        Mary Papenfuss   \n",
       "2  \"Until you have a dog you don't understand wha...         Elyse Wanshel   \n",
       "3  \"Accidentally put grown-up toothpaste on my to...      Caroline Bologna   \n",
       "4  Amy Cooper accused investment firm Franklin Te...        Nina Golgowski   \n",
       "\n",
       "        date  \n",
       "0 2022-09-23  \n",
       "1 2022-09-23  \n",
       "2 2022-09-23  \n",
       "3 2022-09-23  \n",
       "4 2022-09-22  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json(DATA_PATH, lines=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9d6235-621e-4632-a7d0-c08d46dc6f75",
   "metadata": {},
   "source": [
    "##### The dataset originally contains multiple fields such as headline, authors, and link.\n",
    "\n",
    "##### For this project, I only need the text description and the label, so I kept only the columns:\n",
    "\n",
    "##### short_description (the input text)\n",
    "\n",
    "##### category (the target label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afdbf888-5875-4443-85e1-5c10b43c8672",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>short_description</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Health experts said it is too early to predict...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>He was subdued by passengers and crew when he ...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"Until you have a dog you don't understand wha...</td>\n",
       "      <td>COMEDY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Accidentally put grown-up toothpaste on my to...</td>\n",
       "      <td>PARENTING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Amy Cooper accused investment firm Franklin Te...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   short_description   category\n",
       "0  Health experts said it is too early to predict...  U.S. NEWS\n",
       "1  He was subdued by passengers and crew when he ...  U.S. NEWS\n",
       "2  \"Until you have a dog you don't understand wha...     COMEDY\n",
       "3  \"Accidentally put grown-up toothpaste on my to...  PARENTING\n",
       "4  Amy Cooper accused investment firm Franklin Te...  U.S. NEWS"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[[\"short_description\", \"category\"]].copy()\n",
    "\n",
    "df = df.dropna(subset=[\"short_description\"])\n",
    "df[\"short_description\"] = df[\"short_description\"].astype(str).str.strip()\n",
    "df = df[df[\"short_description\"] != \"\"]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85b52ae-0250-4a80-9c14-13508656e332",
   "metadata": {},
   "source": [
    "##### I removed rows with missing or empty short descriptions.\n",
    "\n",
    "##### I also ensured the descriptions were stored as clean strings without unnecessary spaces.\n",
    "\n",
    "##### Filtering categories\n",
    "\n",
    "##### The dataset covers more than 40 categories.\n",
    "\n",
    "##### I restricted it to the four categories required for the task: POLITICS, TRAVEL, SPORTS, and HOME & LIVING."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54edf062-df71-41db-a264-0d59f8c96638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts per category BEFORE balancing:\n",
      "category\n",
      "POLITICS         32441\n",
      "TRAVEL            9421\n",
      "SPORTS            4414\n",
      "HOME & LIVING     4317\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = df[df[\"category\"].isin(TARGET_CATEGORIES)].copy()\n",
    "\n",
    "print(\"Counts per category BEFORE balancing:\")\n",
    "print(df[\"category\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59a41b6-c8d0-4388-97d4-47c2b5350fe4",
   "metadata": {},
   "source": [
    "##### Balancing the dataset\n",
    "\n",
    "##### To keep the data balanced, I sampled 1000 articles from each category, resulting in a total of 4000 records.\n",
    "\n",
    "##### This ensured that each class is equally represented and prevents bias toward larger categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa8d5623-6094-440c-9f8e-f9cf822a73f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts per category AFTER balancing:\n",
      "category\n",
      "HOME & LIVING    1000\n",
      "POLITICS         1000\n",
      "SPORTS           1000\n",
      "TRAVEL           1000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_balanced = (\n",
    "    df.groupby(\"category\", group_keys=False)[[\"short_description\", \"category\"]]\n",
    "      .apply(lambda x: x.sample(n=SAMPLES_PER_CLASS, random_state=RANDOM_STATE))\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(\"Counts per category AFTER balancing:\")\n",
    "print(df_balanced[\"category\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7d6dbe-1b50-4b1b-9c0a-f1b6ebadcae5",
   "metadata": {},
   "source": [
    "##### Splitting into train and test sets\n",
    "\n",
    "##### I performed an 80/20 stratified split:\n",
    "\n",
    "##### Training set: 3200 records\n",
    "\n",
    "##### Test set: 800 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51041643-c456-4e55-b4c9-947c81eeb237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:\n",
      "Train: (3200, 2)\n",
      "Test : (800, 2)\n",
      "\n",
      "Category distribution in Train:\n",
      "category\n",
      "TRAVEL           800\n",
      "SPORTS           800\n",
      "HOME & LIVING    800\n",
      "POLITICS         800\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Category distribution in Test:\n",
      "category\n",
      "HOME & LIVING    200\n",
      "POLITICS         200\n",
      "TRAVEL           200\n",
      "SPORTS           200\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = train_test_split(\n",
    "    df_balanced,\n",
    "    test_size=TEST_SIZE,\n",
    "    stratify=df_balanced[\"category\"],\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(\"Shapes:\")\n",
    "print(\"Train:\", train_df.shape)\n",
    "print(\"Test :\", test_df.shape)\n",
    "\n",
    "print(\"\\nCategory distribution in Train:\")\n",
    "print(train_df[\"category\"].value_counts())\n",
    "\n",
    "print(\"\\nCategory distribution in Test:\")\n",
    "print(test_df[\"category\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42184138-a39b-4061-9efd-389d5b68a3da",
   "metadata": {},
   "source": [
    "##### I saved the balanced dataset and the train/test splits into CSV files, so I can reuse them in later steps without reprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd858c77-4896-40c0-85e1-7b091ef65a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files saved:\n",
      "C:\\Users\\bbuser\\Desktop\\news_keyword_baseline\\train_shortdesc_4cats.csv\n",
      "C:\\Users\\bbuser\\Desktop\\news_keyword_baseline\\test_shortdesc_4cats.csv\n",
      "C:\\Users\\bbuser\\Desktop\\news_keyword_baseline\\balanced_shortdesc_4cats.csv\n"
     ]
    }
   ],
   "source": [
    "train_path = os.path.join(OUTPUT_DIR, \"train_shortdesc_4cats.csv\")\n",
    "test_path  = os.path.join(OUTPUT_DIR, \"test_shortdesc_4cats.csv\")\n",
    "balanced_path = os.path.join(OUTPUT_DIR, \"balanced_shortdesc_4cats.csv\")\n",
    "\n",
    "train_df.to_csv(train_path, index=False, encoding=\"utf-8\")\n",
    "test_df.to_csv(test_path, index=False, encoding=\"utf-8\")\n",
    "df_balanced.to_csv(balanced_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"Files saved:\")\n",
    "print(train_path)\n",
    "print(test_path)\n",
    "print(balanced_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e1fe210-aed4-490e-a1b3-fee249746a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Top 10 words → Accuracy = 0.275\n",
      "TRAVEL: ['the', 'a', 'of', 'to', 'and', 'in', 's', 'you', 'is', 'for']\n",
      "SPORTS: ['the', 'a', 'to', 'of', 'and', 's', 'in', 'it', 'for', 'is']\n",
      "HOME & LIVING: ['the', 'to', 'a', 'and', 'of', 'you', 's', 'in', 'your', 'for']\n",
      "POLITICS: ['the', 'to', 'a', 'of', 'in', 'and', 's', 'is', 'for', 'it']\n",
      "==================================================\n",
      "Top 20 words → Accuracy = 0.333\n",
      "TRAVEL: ['the', 'a', 'of', 'to', 'and', 'in', 's', 'you', 'is', 'for', 'that', 'i', 'it', 'on', 'with', 'are', 'as', 'we', 'but', 't']\n",
      "SPORTS: ['the', 'a', 'to', 'of', 'and', 's', 'in', 'it', 'for', 'is', 'was', 'he', 'on', 'that', 'with', 'at', 'be', 'his', 't', 'i']\n",
      "HOME & LIVING: ['the', 'to', 'a', 'and', 'of', 'you', 's', 'in', 'your', 'for', 'it', 'is', 'we', 'home', 'that', 'this', 'on', 'have', 'with', 'be']\n",
      "POLITICS: ['the', 'to', 'a', 'of', 'in', 'and', 's', 'is', 'for', 'it', 'that', 'on', 'trump', 'he', 'are', 'with', 'his', 'we', 'as', 'this']\n",
      "==================================================\n",
      "Top 30 words → Accuracy = 0.388\n",
      "TRAVEL: ['the', 'a', 'of', 'to', 'and', 'in', 's', 'you', 'is', 'for', 'that', 'i', 'it', 'on', 'with', 'are', 'as', 'we', 'but', 't', 'be', 'an', 'this', 'have', 'your', 'from', 'or', 'at', 'can', 'my']\n",
      "SPORTS: ['the', 'a', 'to', 'of', 'and', 's', 'in', 'it', 'for', 'is', 'was', 'he', 'on', 'that', 'with', 'at', 'be', 'his', 't', 'i', 'but', 'have', 'by', 'has', 'are', 'as', 'this', 'not', 'we', 'after']\n",
      "HOME & LIVING: ['the', 'to', 'a', 'and', 'of', 'you', 's', 'in', 'your', 'for', 'it', 'is', 'we', 'home', 'that', 'this', 'on', 'have', 'with', 'be', 'can', 'are', 't', 'our', 'at', 'but', 'i', 'or', 'from', 'more']\n",
      "POLITICS: ['the', 'to', 'a', 'of', 'in', 'and', 's', 'is', 'for', 'it', 'that', 'on', 'trump', 'he', 'are', 'with', 'his', 'we', 'as', 'this', 'have', 'be', 'has', 'president', 'was', 't', 'not', 'will', 'from', 'by']\n",
      "==================================================\n",
      "Top 50 words → Accuracy = 0.421\n",
      "TRAVEL: ['the', 'a', 'of', 'to', 'and', 'in', 's', 'you', 'is', 'for', 'that', 'i', 'it', 'on', 'with', 'are', 'as', 'we', 'but', 't', 'be', 'an', 'this', 'have', 'your', 'from', 'or', 'at', 'can', 'my', 'was', 'one', 'all', 'some', 'if', 'world', 'travel', 'out', 'not', 'by', 'like', 'when', 'there', 'has', 'up', 'more', 'our', 'about', 'they', 're']\n",
      "SPORTS: ['the', 'a', 'to', 'of', 'and', 's', 'in', 'it', 'for', 'is', 'was', 'he', 'on', 'that', 'with', 'at', 'be', 'his', 't', 'i', 'but', 'have', 'by', 'has', 'are', 'as', 'this', 'not', 'we', 'after', 'game', 'you', 'they', 'an', 'first', 'time', 'all', 'will', 'about', 'new', 'just', 'from', 'said', 'up', 'when', 'nfl', 'over', 'one', 'team', 'football']\n",
      "HOME & LIVING: ['the', 'to', 'a', 'and', 'of', 'you', 's', 'in', 'your', 'for', 'it', 'is', 'we', 'home', 'that', 'this', 'on', 'have', 'with', 'be', 'can', 'are', 't', 'our', 'at', 'but', 'i', 'or', 'from', 'more', 'out', 'by', 'as', 'not', 're', 'one', 'so', 'do', 'all', 'an', 'will', 'if', 'time', 'like', 'new', 'when', 'us', 'these', 'what', 'how']\n",
      "POLITICS: ['the', 'to', 'a', 'of', 'in', 'and', 's', 'is', 'for', 'it', 'that', 'on', 'trump', 'he', 'are', 'with', 'his', 'we', 'as', 'this', 'have', 'be', 'has', 'president', 'was', 't', 'not', 'will', 'from', 'by', 'but', 'an', 'said', 'they', 'about', 'you', 'would', 'i', 'at', 'who', 'their', 'people', 'if', 'up', 'more', 'new', 'donald', 'u', 'been', 'house']\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Tokenizer\n",
    "# -----------------------------\n",
    "def tokenize(text):\n",
    "    \"\"\"Lowercase and extract words only.\"\"\"\n",
    "    return re.findall(r'\\b[a-z]+\\b', text.lower())\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Word frequency per category\n",
    "# -----------------------------\n",
    "category_word_freq = {}\n",
    "for cat in train_df[\"category\"].unique():\n",
    "    texts = train_df[train_df[\"category\"] == cat][\"short_description\"]\n",
    "    words = []\n",
    "    for t in texts:\n",
    "        words.extend(tokenize(t))\n",
    "    category_word_freq[cat] = Counter(words)\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Classification function\n",
    "# -----------------------------\n",
    "def classify(text, word_sets):\n",
    "    words = set(tokenize(text))\n",
    "    overlaps = {cat: len(words & wset) for cat, wset in word_sets.items()}\n",
    "    \n",
    "    max_overlap = max(overlaps.values())\n",
    "    if max_overlap == 0:\n",
    "        return \"UNKNOWN\"\n",
    "    \n",
    "    candidates = [cat for cat, val in overlaps.items() if val == max_overlap]\n",
    "    return candidates[0]   # simple tie-breaking\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Evaluation\n",
    "# -----------------------------\n",
    "def evaluate(word_sets):\n",
    "    correct, total = 0, 0\n",
    "    for _, row in test_df.iterrows():\n",
    "        pred = classify(row[\"short_description\"], word_sets)\n",
    "        if pred == row[\"category\"]:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    return correct / total\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Run experiments (with frequency printing)\n",
    "# -----------------------------\n",
    "results = {}\n",
    "for n in [10, 20, 30, 50]:\n",
    "    word_sets = {cat: set([w for w, _ in counter.most_common(n)]) \n",
    "                 for cat, counter in category_word_freq.items()}\n",
    "    acc = evaluate(word_sets)\n",
    "    results[n] = acc\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Top {n} words → Accuracy = {acc:.3f}\")\n",
    "    \n",
    "    # show top-n frequent words per category\n",
    "    for cat, counter in category_word_freq.items():\n",
    "        top_words = [w for w, _ in counter.most_common(n)]\n",
    "        print(f\"{cat}: {top_words}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a905109f-a15a-496d-b3b1-5e04977e0cd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
