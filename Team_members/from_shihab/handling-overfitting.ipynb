{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc63f208-f33e-4524-a237-b2ff81bceca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1 — Load & prepare CIFAR-10 (flatten to vectors)\n",
    "import time\n",
    "import numpy as np\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ef98d5-3349-4c4d-879a-ada4b5acefe4",
   "metadata": {},
   "source": [
    "# Load CIFAR-10: images (32x32x3), labels 0..9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58941cbf-24e3-4ad1-bd37-29fbcf2e796f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 0us/step\n",
      "(50000, 3072) (50000,) (10000, 3072) (10000,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "y_train = y_train.ravel()\n",
    "y_test  = y_test.ravel()\n",
    "\n",
    "# Flatten images to 3072-length vectors and scale to [0,1]\n",
    "x_train = x_train.reshape(len(x_train), -1).astype(\"float32\") / 255.0\n",
    "x_test  = x_test.reshape(len(x_test),  -1).astype(\"float32\") / 255.0\n",
    "\n",
    "input_dim = x_train.shape[1]   # 32*32*3 = 3072\n",
    "num_classes = 10\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30371e2d-044f-465d-86c7-25f8c75012d4",
   "metadata": {},
   "source": [
    "# STEP 2 — Baseline MLP (no regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "babde1b6-30b0-415c-aa14-dfae03a70a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASELINE  | best_val_acc=0.5214 at epoch=29 | test_acc=0.5155 | time=535.4s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def build_baseline(input_dim, num_classes=10):\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Input(shape=(input_dim,)),\n",
    "        keras.layers.Dense(512, activation=\"relu\"),\n",
    "        keras.layers.Dense(256, activation=\"relu\"),\n",
    "        keras.layers.Dense(num_classes, activation=\"softmax\")\n",
    "    ])\n",
    "    model.compile(optimizer=\"adam\",\n",
    "                  loss=\"sparse_categorical_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "baseline = build_baseline(input_dim)\n",
    "t0 = time.perf_counter()\n",
    "hist_base = baseline.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=30, batch_size=128, validation_split=0.1, verbose=0\n",
    ")\n",
    "time_base = time.perf_counter() - t0\n",
    "\n",
    "# Best validation accuracy and its epoch (1-based)\n",
    "val_accs = hist_base.history[\"val_accuracy\"]\n",
    "best_val_acc_base = float(np.max(val_accs))\n",
    "best_epoch_base = int(np.argmax(val_accs) + 1)\n",
    "\n",
    "test_acc_base = float(baseline.evaluate(x_test, y_test, verbose=0)[1])\n",
    "\n",
    "print(f\"BASELINE  | best_val_acc={best_val_acc_base:.4f} at epoch={best_epoch_base} | \"\n",
    "      f\"test_acc={test_acc_base:.4f} | time={time_base:.1f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30fce4d-b500-48a9-a1df-de0bdff54f16",
   "metadata": {},
   "source": [
    "# STEP 3 — MLP with Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5f09660-eced-47ef-a338-8c28235281a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPOUT   | best_val_acc=0.3552 at epoch=28 | test_acc=0.3471 | time=567.7s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def build_dropout(input_dim, num_classes=10, p=0.5):\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Input(shape=(input_dim,)),\n",
    "        keras.layers.Dense(512, activation=\"relu\"),\n",
    "        keras.layers.Dropout(p),\n",
    "        keras.layers.Dense(256, activation=\"relu\"),\n",
    "        keras.layers.Dropout(p),\n",
    "        keras.layers.Dense(num_classes, activation=\"softmax\")\n",
    "    ])\n",
    "    model.compile(optimizer=\"adam\",\n",
    "                  loss=\"sparse_categorical_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "drop = build_dropout(input_dim, p=0.5)\n",
    "t0 = time.perf_counter()\n",
    "hist_drop = drop.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=30, batch_size=128, validation_split=0.1, verbose=0\n",
    ")\n",
    "time_drop = time.perf_counter() - t0\n",
    "\n",
    "val_accs = hist_drop.history[\"val_accuracy\"]\n",
    "best_val_acc_drop = float(np.max(val_accs))\n",
    "best_epoch_drop = int(np.argmax(val_accs) + 1)\n",
    "test_acc_drop = float(drop.evaluate(x_test, y_test, verbose=0)[1])\n",
    "\n",
    "print(f\"DROPOUT   | best_val_acc={best_val_acc_drop:.4f} at epoch={best_epoch_drop} | \"\n",
    "      f\"test_acc={test_acc_drop:.4f} | time={time_drop:.1f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf212c9-d18d-4e32-b4bf-0fc1da8595f2",
   "metadata": {},
   "source": [
    "# STEP 4 — MLP with L2 weight regularization (kernel_regularizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56c0f74e-424a-4623-8c38-8c8b328ea5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2(1e-4)  | best_val_acc=0.5330 at epoch=22 | test_acc=0.5167 | time=547.5s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "def build_l2(input_dim, num_classes=10, l2=1e-4):\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Input(shape=(input_dim,)),\n",
    "        keras.layers.Dense(512, activation=\"relu\",\n",
    "                           kernel_regularizer=regularizers.l2(l2)),\n",
    "        keras.layers.Dense(256, activation=\"relu\",\n",
    "                           kernel_regularizer=regularizers.l2(l2)),\n",
    "        keras.layers.Dense(num_classes, activation=\"softmax\")\n",
    "    ])\n",
    "    model.compile(optimizer=\"adam\",\n",
    "                  loss=\"sparse_categorical_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "l2m = build_l2(input_dim, l2=1e-4)\n",
    "t0 = time.perf_counter()\n",
    "hist_l2 = l2m.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=30, batch_size=128, validation_split=0.1, verbose=0\n",
    ")\n",
    "time_l2 = time.perf_counter() - t0\n",
    "\n",
    "val_accs = hist_l2.history[\"val_accuracy\"]\n",
    "best_val_acc_l2 = float(np.max(val_accs))\n",
    "best_epoch_l2 = int(np.argmax(val_accs) + 1)\n",
    "test_acc_l2 = float(l2m.evaluate(x_test, y_test, verbose=0)[1])\n",
    "\n",
    "print(f\"L2(1e-4)  | best_val_acc={best_val_acc_l2:.4f} at epoch={best_epoch_l2} | \"\n",
    "      f\"test_acc={test_acc_l2:.4f} | time={time_l2:.1f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7a68cb-43f5-47a0-a62b-428ea7a49ba3",
   "metadata": {},
   "source": [
    "# STEP 5 — Combine Dropout + L2 + EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "892650ab-9db4-4526-8448-a983ec882361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROP+L2+ES| best_val_acc=0.3274 at epoch=6 | test_acc=0.3259 | time=206.7s (early stopping)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def build_drop_l2(input_dim, num_classes=10, p=0.5, l2=1e-4):\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Input(shape=(input_dim,)),\n",
    "        keras.layers.Dense(512, activation=\"relu\",\n",
    "                           kernel_regularizer=regularizers.l2(l2)),\n",
    "        keras.layers.Dropout(p),\n",
    "        keras.layers.Dense(256, activation=\"relu\",\n",
    "                           kernel_regularizer=regularizers.l2(l2)),\n",
    "        keras.layers.Dropout(p),\n",
    "        keras.layers.Dense(num_classes, activation=\"softmax\")\n",
    "    ])\n",
    "    model.compile(optimizer=\"adam\",\n",
    "                  loss=\"sparse_categorical_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "combo = build_drop_l2(input_dim, p=0.5, l2=1e-4)\n",
    "\n",
    "early = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_accuracy\", mode=\"max\",\n",
    "    patience=5, restore_best_weights=True\n",
    ")\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "hist_combo = combo.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=50, batch_size=128, validation_split=0.1,\n",
    "    callbacks=[early], verbose=0\n",
    ")\n",
    "time_combo = time.perf_counter() - t0\n",
    "\n",
    "val_accs = hist_combo.history[\"val_accuracy\"]\n",
    "best_val_acc_combo = float(np.max(val_accs))\n",
    "best_epoch_combo = int(np.argmax(val_accs) + 1)\n",
    "test_acc_combo = float(combo.evaluate(x_test, y_test, verbose=0)[1])\n",
    "\n",
    "print(f\"DROP+L2+ES| best_val_acc={best_val_acc_combo:.4f} at epoch={best_epoch_combo} | \"\n",
    "      f\"test_acc={test_acc_combo:.4f} | time={time_combo:.1f}s (early stopping)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b69e2bd-a751-463d-b43d-ed5bc19db2ff",
   "metadata": {},
   "source": [
    "# STEP 6 — Final commparing tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da299485-e1d7-46a3-9bab-6cc1cef5bc89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CIFAR-10 DenseNN — Regularization Comparison ===\n",
      "        Model  Best Val Acc  Best Epoch  Test Acc    Time (s)\n",
      "0    BASELINE        0.5214          29    0.5155  535.355442\n",
      "1     DROPOUT        0.3552          28    0.3471  567.685718\n",
      "2   L2 (1e-4)        0.5330          22    0.5167  547.515501\n",
      "3  DROP+L2+ES        0.3274           6    0.3259  206.710857\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "summary = pd.DataFrame([\n",
    "    {\"Model\":\"BASELINE\",     \"Best Val Acc\":best_val_acc_base,  \"Best Epoch\":best_epoch_base,  \"Test Acc\":test_acc_base,  \"Time (s)\":time_base},\n",
    "    {\"Model\":\"DROPOUT\",      \"Best Val Acc\":best_val_acc_drop,  \"Best Epoch\":best_epoch_drop,  \"Test Acc\":test_acc_drop,  \"Time (s)\":time_drop},\n",
    "    {\"Model\":\"L2 (1e-4)\",    \"Best Val Acc\":best_val_acc_l2,    \"Best Epoch\":best_epoch_l2,    \"Test Acc\":test_acc_l2,    \"Time (s)\":time_l2},\n",
    "    {\"Model\":\"DROP+L2+ES\",   \"Best Val Acc\":best_val_acc_combo, \"Best Epoch\":best_epoch_combo, \"Test Acc\":test_acc_combo, \"Time (s)\":time_combo},\n",
    "])\n",
    "print(\"\\n=== CIFAR-10 DenseNN — Regularization Comparison ===\")\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe02fd51-306f-4e9e-989c-e954263e784a",
   "metadata": {},
   "source": [
    "# q1 - How does adding dropout layers affect training vs validation accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8f23fd-cba7-45ee-90bf-7867d39340f4",
   "metadata": {},
   "source": [
    "In the results, Dropout alone lowered both validation and test accuracy compared to the baseline.\n",
    "\n",
    "Baseline Val Acc: 0.5214, Test Acc: 0.5155\n",
    "\n",
    "Dropout Val Acc: 0.3552, Test Acc: 0.3471\n",
    "\n",
    "This shows that Dropout made the network underfit in this setup — the model lost capacity and could not learn strong patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cc782d-64fc-4f26-8b65-be341b42d090",
   "metadata": {},
   "source": [
    "# q2- Does early stopping prevent wasted training time?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ca8076-c0a8-4557-a700-36cb14169dc1",
   "metadata": {},
   "source": [
    "Yes. With Dropout + L2 + EarlyStopping, training stopped early at epoch 6 instead of running full 30 epochs.\n",
    "\n",
    "Training time dropped to 206s compared to ~535–567s for other models.\n",
    "\n",
    "However, accuracy was lower (Val Acc 0.3274, Test Acc 0.3259) because regularization was very strong.\n",
    "\n",
    "So, early stopping saves time, but combined with heavy regularization it can lead to underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f411aa93-d9e7-42a8-9b72-e2eddf302b6b",
   "metadata": {},
   "source": [
    "# q3- Can L2 weight regularization improve generalization?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a226b61-9041-44c3-9a40-b8d76a07f463",
   "metadata": {},
   "source": [
    "Yes, slightly. the L2 model achieved the highest validation accuracy (0.5330) and a slightly better test accuracy (0.5167) than the baseline.\n",
    "\n",
    "This suggests L2 weight decay improved generalization a little, by reducing overfitting without hurting performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b544bc7-3c69-44ea-9fb7-2682f35e55cc",
   "metadata": {},
   "source": [
    "# q4- How does model depth affect overfitting on CIFAR-10?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb54a86-7dd9-45ac-bf6a-7db545911947",
   "metadata": {},
   "source": [
    "In the experiments, model depth was kept constant (2 hidden layers).\n",
    "\n",
    "So the results here don’t show depth effects directly.\n",
    "\n",
    "But in general: deeper models often overfit on CIFAR-10 with dense layers, which is why regularization techniques (Dropout, L2, EarlyStopping) are tested."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0466e45b-81cc-45bb-be67-7644b7c3f64d",
   "metadata": {},
   "source": [
    "# In summary:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43eceecf-0d7e-4bb5-a269-6c40469f6d86",
   "metadata": {},
   "source": [
    "Dropout hurt performance in this dense model.\n",
    "\n",
    "EarlyStopping reduced wasted time but combined with heavy regularization caused underfitting.\n",
    "\n",
    "L2 gave the best overall balance (slight accuracy improvement, stable validation accuracy).\n",
    "\n",
    "Model depth was not varied here, so no conclusion about depth from your run."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mlenv)",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
