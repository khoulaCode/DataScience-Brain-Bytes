{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6259955",
   "metadata": {},
   "source": [
    "## Domain-Specific Dataset Exploration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "796eacc6",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 67\u001b[39m\n\u001b[32m     61\u001b[39m X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=\u001b[32m0.5\u001b[39m, random_state=\u001b[32m42\u001b[39m, stratify=y_temp)\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# Preprocess: OneHot(cats) + Scale(nums)\u001b[39;00m\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m ohe = OneHotEncoder(handle_unknown=\u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m, sparse=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     68\u001b[39m pre = ColumnTransformer(\n\u001b[32m     69\u001b[39m     transformers=[\n\u001b[32m     70\u001b[39m         (\u001b[33m\"\u001b[39m\u001b[33mcat\u001b[39m\u001b[33m\"\u001b[39m, ohe, cat_cols),\n\u001b[32m   (...)\u001b[39m\u001b[32m     74\u001b[39m     verbose_feature_names_out=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     75\u001b[39m )\n\u001b[32m     77\u001b[39m \u001b[38;5;66;03m# Fit on train only\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'"
     ]
    }
   ],
   "source": [
    "# adult_classical_vs_nn.py\n",
    "# Classical ML (LogReg + RandomForest) vs Feed-forward Neural Net on UCI Adult (Census Income)\n",
    "\n",
    "import os, io, time, urllib.request, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "\n",
    "# -----------------------------\n",
    "# Dataset (UCI Adult / Census Income)\n",
    "# Source: UCI ML Repo (adult.data, adult.test)\n",
    "# https://archive.ics.uci.edu/dataset/2/adult\n",
    "# -----------------------------\n",
    "COLS = [\n",
    "    \"age\",\"workclass\",\"fnlwgt\",\"education\",\"education-num\",\"marital-status\",\n",
    "    \"occupation\",\"relationship\",\"race\",\"sex\",\"capital-gain\",\"capital-loss\",\n",
    "    \"hours-per-week\",\"native-country\",\"income\"\n",
    "]\n",
    "DATA_URL = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
    "TEST_URL = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\"\n",
    "\n",
    "def fetch_csv(url, names, skiprows=0):\n",
    "    raw = urllib.request.urlopen(url).read()\n",
    "    return pd.read_csv(io.BytesIO(raw), header=None, names=names, skiprows=skiprows, na_values=[\"?\",\" ?\"])\n",
    "\n",
    "df_train = fetch_csv(DATA_URL, COLS, skiprows=0)\n",
    "df_test  = fetch_csv(TEST_URL, COLS, skiprows=1)  # adult.test has a header comment line\n",
    "\n",
    "# Clean labels: remove trailing periods in test labels and strip spaces\n",
    "df_train[\"income\"] = df_train[\"income\"].astype(str).str.strip()\n",
    "df_test[\"income\"]  = df_test[\"income\"].astype(str).str.replace(\".\", \"\", regex=False).str.strip()\n",
    "\n",
    "# Concatenate & shuffle once (we’ll re-split with stratification)\n",
    "df = pd.concat([df_train, df_test], ignore_index=True)\n",
    "# Replace NaN (from '?') with 'Unknown' for categorical columns\n",
    "for c in df.columns:\n",
    "    if df[c].dtype == object:\n",
    "        df[c] = df[c].fillna(\"Unknown\").str.strip()\n",
    "\n",
    "# Target to binary\n",
    "df[\"income\"] = (df[\"income\"] == \">50K\").astype(int)\n",
    "\n",
    "# Features / target\n",
    "X = df.drop(columns=[\"income\"])\n",
    "y = df[\"income\"].values\n",
    "\n",
    "# Identify column types\n",
    "num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = [c for c in X.columns if c not in num_cols]\n",
    "\n",
    "# Time-ordered split isn’t relevant here; we do a stratified split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "# -----------------------------\n",
    "# Preprocess: OneHot(cats) + Scale(nums)\n",
    "# -----------------------------\n",
    "\n",
    "ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "pre = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", ohe, cat_cols),\n",
    "        (\"num\", StandardScaler(), num_cols),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    "    verbose_feature_names_out=False,\n",
    ")\n",
    "\n",
    "# Fit on train only\n",
    "pre.fit(X_train)\n",
    "Xtr = pre.transform(X_train)\n",
    "Xva = pre.transform(X_val)\n",
    "Xte = pre.transform(X_test)\n",
    "\n",
    "def metrics(y_true, y_prob, thr=0.5):\n",
    "    y_pred = (y_prob >= thr).astype(int)\n",
    "    out = {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"f1\": f1_score(y_true, y_pred, zero_division=0),\n",
    "        \"auroc\": roc_auc_score(y_true, y_prob),\n",
    "    }\n",
    "    return out\n",
    "\n",
    "def show(name, train_m, test_m, tsec):\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(f\"Train time: {tsec:.2f}s\")\n",
    "    print(\"Train:\", {k: round(v,4) for k,v in train_m.items()})\n",
    "    print(\"Test :\", {k: round(v,4) for k,v in test_m.items()})\n",
    "\n",
    "results = {}\n",
    "\n",
    "# -----------------------------\n",
    "# Classical 1: Logistic Regression (balanced)\n",
    "# -----------------------------\n",
    "logreg = LogisticRegression(max_iter=2000, solver=\"lbfgs\", class_weight=\"balanced\")\n",
    "t0 = time.perf_counter()\n",
    "logreg.fit(Xtr, y_train)\n",
    "t1 = time.perf_counter()\n",
    "train_prob = logreg.predict_proba(Xtr)[:,1]\n",
    "test_prob  = logreg.predict_proba(Xte)[:,1]\n",
    "res_tr = metrics(y_train, train_prob)\n",
    "res_te = metrics(y_test,  test_prob)\n",
    "show(\"Logistic Regression\", res_tr, res_te, t1 - t0)\n",
    "results[\"logreg\"] = {\"train\": res_tr, \"test\": res_te, \"time_s\": t1 - t0}\n",
    "\n",
    "# -----------------------------\n",
    "# Classical 2: Random Forest\n",
    "# -----------------------------\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=400, max_depth=None, random_state=42, n_jobs=-1, class_weight=\"balanced_subsample\"\n",
    ")\n",
    "t0 = time.perf_counter()\n",
    "rf.fit(Xtr, y_train)\n",
    "t1 = time.perf_counter()\n",
    "train_prob = rf.predict_proba(Xtr)[:,1]\n",
    "test_prob  = rf.predict_proba(Xte)[:,1]\n",
    "res_tr = metrics(y_train, train_prob)\n",
    "res_te = metrics(y_test,  test_prob)\n",
    "show(\"Random Forest\", res_tr, res_te, t1 - t0)\n",
    "results[\"random_forest\"] = {\"train\": res_tr, \"test\": res_te, \"time_s\": t1 - t0}\n",
    "\n",
    "# -----------------------------\n",
    "# Neural Network (Feed-forward MLP)\n",
    "# -----------------------------\n",
    "def build_mlp(input_dim, width=(256,128), dropout=0.3, l2=1e-4):\n",
    "    reg = regularizers.l2(l2) if l2 else None\n",
    "    inp = keras.Input(shape=(input_dim,))\n",
    "    x = inp\n",
    "    for w in width:\n",
    "        x = layers.Dense(w, kernel_regularizer=reg)(x)\n",
    "        x = layers.ReLU()(x)\n",
    "        x = layers.Dropout(dropout)(x)\n",
    "    out = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inp, out)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(1e-3),\n",
    "                  loss=\"binary_crossentropy\",\n",
    "                  metrics=[keras.metrics.AUC(name=\"auc\")])\n",
    "    return model\n",
    "\n",
    "mlp = build_mlp(Xtr.shape[1], width=(256,128), dropout=0.3, l2=1e-4)\n",
    "es = keras.callbacks.EarlyStopping(monitor=\"val_auc\", mode=\"max\", patience=6, restore_best_weights=True)\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "hist = mlp.fit(\n",
    "    Xtr, y_train.astype(\"float32\"),\n",
    "    validation_data=(Xva, y_val.astype(\"float32\")),\n",
    "    epochs=50, batch_size=512, verbose=0, callbacks=[es]\n",
    ")\n",
    "t1 = time.perf_counter()\n",
    "\n",
    "train_prob = mlp.predict(Xtr, verbose=0).ravel()\n",
    "test_prob  = mlp.predict(Xte, verbose=0).ravel()\n",
    "res_tr = metrics(y_train, train_prob)\n",
    "res_te = metrics(y_test,  test_prob)\n",
    "best_epoch = int(np.argmax(hist.history[\"val_auc\"]) + 1)\n",
    "\n",
    "show(\"Neural Net (MLP)\", res_tr, res_te, t1 - t0)\n",
    "print(f\"Best val AUC epoch: {best_epoch}\")\n",
    "results[\"mlp\"] = {\"train\": res_tr, \"test\": res_te, \"time_s\": t1 - t0, \"best_epoch\": best_epoch}\n",
    "\n",
    "# -----------------------------\n",
    "# Save & plot quick comparison\n",
    "# -----------------------------\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "pd.Series(results).to_json(\"outputs/results.json\")\n",
    "print(\"\\nSaved: outputs/results.json\")\n",
    "\n",
    "# Bar plot: Test AUROC & F1\n",
    "labels = [\"LogReg\",\"RandomForest\",\"MLP\"]\n",
    "aucs = [results[\"logreg\"][\"test\"][\"auroc\"], results[\"random_forest\"][\"test\"][\"auroc\"], results[\"mlp\"][\"test\"][\"auroc\"]]\n",
    "f1s  = [results[\"logreg\"][\"test\"][\"f1\"],   results[\"random_forest\"][\"test\"][\"f1\"],   results[\"mlp\"][\"test\"][\"f1\"]]\n",
    "\n",
    "plt.figure()\n",
    "x = np.arange(len(labels))\n",
    "w = 0.35\n",
    "plt.bar(x - w/2, aucs, width=w, label=\"AUROC\")\n",
    "plt.bar(x + w/2, f1s,  width=w, label=\"F1\")\n",
    "plt.xticks(x, labels)\n",
    "plt.title(\"Adult Income — Test AUROC & F1\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim(0,1)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"outputs/test_scores.png\", dpi=160)\n",
    "print(\"Saved: outputs/test_scores.png\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
