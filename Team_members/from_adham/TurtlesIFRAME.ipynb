{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95c297cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ee124ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_url = 'https://www.scrapethissite.com/pages/frames/?frame=i'\n",
    "base_url = 'https://www.scrapethissite.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c099337d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Access granted to the page.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "response = requests.get(page_url)\n",
    "if response.status_code == 200:\n",
    "    print(\"Access granted to the page.\")\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "else:\n",
    "    print(f\"Failed to access the page. Status code: {response.status_code}\")\n",
    "    exit()\n",
    "\n",
    "turtles = soup.find_all('div', class_='col-md-4 turtle-family-card')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85781ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14 turtle families to process\n",
      "Processing turtle 1/14...\n",
      "Processing turtle 2/14...\n",
      "Processing turtle 2/14...\n",
      "Processing turtle 3/14...\n",
      "Processing turtle 3/14...\n",
      "Processing turtle 4/14...\n",
      "Processing turtle 4/14...\n",
      "Processing turtle 5/14...\n",
      "Processing turtle 5/14...\n",
      "Processing turtle 6/14...\n",
      "Processing turtle 6/14...\n",
      "Processing turtle 7/14...\n",
      "Processing turtle 7/14...\n",
      "Processing turtle 8/14...\n",
      "Processing turtle 8/14...\n",
      "Processing turtle 9/14...\n",
      "Processing turtle 9/14...\n",
      "Processing turtle 10/14...\n",
      "Processing turtle 10/14...\n",
      "Processing turtle 11/14...\n",
      "Processing turtle 11/14...\n",
      "Processing turtle 12/14...\n",
      "Processing turtle 12/14...\n",
      "Processing turtle 13/14...\n",
      "Processing turtle 13/14...\n",
      "Processing turtle 14/14...\n",
      "Processing turtle 14/14...\n",
      "\n",
      "Scraping completed!\n",
      "Collected data for 14 turtle families\n",
      "\n",
      "Scraping completed!\n",
      "Collected data for 14 turtle families\n"
     ]
    }
   ],
   "source": [
    "# Initialize empty dictionary to store turtle data\n",
    "turtles_data = {\n",
    "    \"Name\": [],\n",
    "    \"Known_As\": [],\n",
    "    \"Discovery_Year\": [],\n",
    "    \"Discovered_By\": []\n",
    "}\n",
    "\n",
    "# Add a counter to track progress\n",
    "total_turtles = len(turtles)\n",
    "print(f\"Found {total_turtles} turtle families to process\")\n",
    "\n",
    "# Process each turtle family\n",
    "for i, turtle in enumerate(turtles):\n",
    "    # Display progress\n",
    "    print(f\"Processing turtle {i+1}/{total_turtles}...\")\n",
    "    \n",
    "    # STEP 1: Extract the turtle family name\n",
    "    name_tag = turtle.find('h3', class_='family-name')\n",
    "    if name_tag:\n",
    "        name = name_tag.text.strip()\n",
    "        turtles_data['Name'].append(name)\n",
    "    else:\n",
    "        turtles_data['Name'].append(\"Unknown\")\n",
    "        print(f\"  Warning: Could not find name for turtle #{i+1}\")\n",
    "\n",
    "    # STEP 2: Find and follow the detail link\n",
    "    link_tag = turtle.find('a', href=True)\n",
    "    if link_tag:\n",
    "        # Create the full URL and request the detail page\n",
    "        detail_url = base_url + link_tag['href']\n",
    "        \n",
    "        try:\n",
    "            # Add basic error handling for network requests\n",
    "            detail_response = requests.get(detail_url, timeout=10)\n",
    "            detail_response.raise_for_status()  # Will raise an exception for 4XX/5XX responses\n",
    "            \n",
    "            # Parse the detail page HTML\n",
    "            detail_soup = BeautifulSoup(detail_response.content, 'html.parser')\n",
    "            \n",
    "            # Extract the lead paragraph with detailed information\n",
    "            info = detail_soup.find('p', class_='lead')\n",
    "\n",
    "            if info:\n",
    "                # STEP 3: Extract the common name\n",
    "                known_tag = info.find('strong', class_='common-name')\n",
    "                known_as = known_tag.text.strip() if known_tag else \"Unknown\"\n",
    "                turtles_data['Known_As'].append(known_as)\n",
    "\n",
    "                # STEP 4: Extract the discovery year using regex\n",
    "                # Look for 4-digit years between 1700-2099\n",
    "                year_match = re.search(r'\\b(1[7-9]\\d{2}|20\\d{2})\\b', info.text)\n",
    "                year = year_match.group(0) if year_match else \"Unknown\"\n",
    "                turtles_data['Discovery_Year'].append(year)\n",
    "\n",
    "                # STEP 5: Extract the discoverer name using regex\n",
    "                # Look for text that follows \"by \" and starts with a capital letter\n",
    "                discoverer_match = re.search(r'by ([A-Z][a-zA-Z\\s\\.\\-]*)', info.text)\n",
    "                discoverer = discoverer_match.group(1).strip() if discoverer_match else \"Unknown\"\n",
    "                turtles_data['Discovered_By'].append(discoverer)\n",
    "            else:\n",
    "                # Handle missing information\n",
    "                turtles_data['Known_As'].append(\"Unknown\")\n",
    "                turtles_data['Discovery_Year'].append(\"Unknown\")\n",
    "                turtles_data['Discovered_By'].append(\"Unknown\")\n",
    "                print(f\"  Warning: No detailed information found for {name}\")\n",
    "        \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            # Handle any request errors (timeout, connection errors, etc.)\n",
    "            turtles_data['Known_As'].append(\"Error\")\n",
    "            turtles_data['Discovery_Year'].append(\"Error\")\n",
    "            turtles_data['Discovered_By'].append(\"Error\")\n",
    "            print(f\"  Error accessing {detail_url}: {e}\")\n",
    "    else:\n",
    "        # Handle missing link\n",
    "        turtles_data['Known_As'].append(\"No Link\")\n",
    "        turtles_data['Discovery_Year'].append(\"No Link\")\n",
    "        turtles_data['Discovered_By'].append(\"No Link\")\n",
    "        print(f\"  Warning: No detail link found for turtle #{i+1}\")\n",
    "\n",
    "print(\"\\nScraping completed!\")\n",
    "print(f\"Collected data for {len(turtles_data['Name'])} turtle families\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa91d17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Known_As</th>\n",
       "      <th>Discovery_Year</th>\n",
       "      <th>Discovered_By</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Carettochelyidae</td>\n",
       "      <td>Pig-nosed turtle</td>\n",
       "      <td>1887</td>\n",
       "      <td>Boulenger.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cheloniidae</td>\n",
       "      <td>Sea turtles</td>\n",
       "      <td>1811</td>\n",
       "      <td>Oppel.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chelydridae</td>\n",
       "      <td>Snapping turtles</td>\n",
       "      <td>1831</td>\n",
       "      <td>Gray.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dermatemydidae</td>\n",
       "      <td>Central American river turtle</td>\n",
       "      <td>1870</td>\n",
       "      <td>Gray.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dermochelyidae</td>\n",
       "      <td>Leatherback sea turtle</td>\n",
       "      <td>1843</td>\n",
       "      <td>Fitzinger.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Emydidae</td>\n",
       "      <td>Pond or water turtles</td>\n",
       "      <td>1815</td>\n",
       "      <td>Rafinesque.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Geoemydidae</td>\n",
       "      <td>Asian river, leaf, roofed or Asian box turtles</td>\n",
       "      <td>1868</td>\n",
       "      <td>Theobald.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Kinosternidae</td>\n",
       "      <td>Mud or musk turtles</td>\n",
       "      <td>1857</td>\n",
       "      <td>Agassiz.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Platysternidae</td>\n",
       "      <td>Big-headed turtle</td>\n",
       "      <td>1869</td>\n",
       "      <td>Gray.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Testudinidae</td>\n",
       "      <td>Tortoises</td>\n",
       "      <td>1788</td>\n",
       "      <td>Batsch.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Trionychidae</td>\n",
       "      <td>Softshell turtles</td>\n",
       "      <td>1826</td>\n",
       "      <td>Fitzinger.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Chelidae</td>\n",
       "      <td>Austro-American sideneck turtles</td>\n",
       "      <td>1831</td>\n",
       "      <td>Gray.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Pelomedusidae</td>\n",
       "      <td>Afro-American sideneck turtles</td>\n",
       "      <td>1868</td>\n",
       "      <td>Cope.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Podocnemididae</td>\n",
       "      <td>Madagascar big-headed, Big-headed Amazon River...</td>\n",
       "      <td>1869</td>\n",
       "      <td>Gray.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Name                                           Known_As  \\\n",
       "0   Carettochelyidae                                   Pig-nosed turtle   \n",
       "1        Cheloniidae                                        Sea turtles   \n",
       "2        Chelydridae                                   Snapping turtles   \n",
       "3     Dermatemydidae                      Central American river turtle   \n",
       "4     Dermochelyidae                             Leatherback sea turtle   \n",
       "5           Emydidae                              Pond or water turtles   \n",
       "6        Geoemydidae     Asian river, leaf, roofed or Asian box turtles   \n",
       "7      Kinosternidae                                Mud or musk turtles   \n",
       "8     Platysternidae                                  Big-headed turtle   \n",
       "9       Testudinidae                                          Tortoises   \n",
       "10      Trionychidae                                  Softshell turtles   \n",
       "11          Chelidae                   Austro-American sideneck turtles   \n",
       "12     Pelomedusidae                     Afro-American sideneck turtles   \n",
       "13    Podocnemididae  Madagascar big-headed, Big-headed Amazon River...   \n",
       "\n",
       "   Discovery_Year Discovered_By  \n",
       "0            1887    Boulenger.  \n",
       "1            1811        Oppel.  \n",
       "2            1831         Gray.  \n",
       "3            1870         Gray.  \n",
       "4            1843    Fitzinger.  \n",
       "5            1815   Rafinesque.  \n",
       "6            1868     Theobald.  \n",
       "7            1857      Agassiz.  \n",
       "8            1869         Gray.  \n",
       "9            1788       Batsch.  \n",
       "10           1826    Fitzinger.  \n",
       "11           1831         Gray.  \n",
       "12           1868         Cope.  \n",
       "13           1869         Gray.  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DataFrame from the collected data\n",
    "df = pd.DataFrame(turtles_data)\n",
    "\n",
    "# Add a count column to make it easier to reference rows\n",
    "df['ID'] = range(1, len(df) + 1)\n",
    "\n",
    "# Reorder columns to put ID first\n",
    "df = df[['ID', 'Name', 'Known_As', 'Discovery_Year', 'Discovered_By']]\n",
    "\n",
    "# Display summary statistics\n",
    "print(f\"Total rows: {len(df)}\")\n",
    "print(f\"Missing values: {df.isna().sum().sum()}\")\n",
    "\n",
    "# Replace empty strings or None values with \"Unknown\"\n",
    "df = df.fillna(\"Unknown\")\n",
    "\n",
    "# Set display options to show all rows\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Display the full DataFrame\n",
    "print(\"\\nTurtle Families Data:\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ca136a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to turtles_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Import datetime for timestamping files\n",
    "from datetime import datetime\n",
    "\n",
    "# Get current date and time for filename\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "csv_filename = f'turtles_data_{timestamp}.csv'\n",
    "\n",
    "# Save to CSV with index=False to exclude row numbers\n",
    "df.to_csv(csv_filename, index=False)\n",
    "print(f\"Data saved to {csv_filename}\")\n",
    "\n",
    "# OPTIONAL: Save to Excel format with formatting (if xlsxwriter is installed)\n",
    "try:\n",
    "    excel_filename = f'turtles_data_{timestamp}.xlsx'\n",
    "    \n",
    "    # Create a writer for Excel\n",
    "    with pd.ExcelWriter(excel_filename, engine='xlsxwriter') as writer:\n",
    "        # Write the dataframe to Excel\n",
    "        df.to_excel(writer, sheet_name='Turtle Families', index=False)\n",
    "        \n",
    "        # Get the xlsxwriter workbook and worksheet objects\n",
    "        workbook = writer.book\n",
    "        worksheet = writer.sheets['Turtle Families']\n",
    "        \n",
    "        # Add a header format\n",
    "        header_format = workbook.add_format({\n",
    "            'bold': True,\n",
    "            'text_wrap': True,\n",
    "            'valign': 'top',\n",
    "            'bg_color': '#D8E4BC',\n",
    "            'border': 1\n",
    "        })\n",
    "        \n",
    "        # Apply the header format to the header row\n",
    "        for col_num, value in enumerate(df.columns.values):\n",
    "            worksheet.write(0, col_num, value, header_format)\n",
    "            \n",
    "        # Auto-adjust columns' width\n",
    "        for i, col in enumerate(df.columns):\n",
    "            column_width = max(df[col].astype(str).map(len).max(), len(col)) + 2\n",
    "            worksheet.set_column(i, i, column_width)\n",
    "    \n",
    "    print(f\"Data also saved to Excel: {excel_filename}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not save to Excel format: {e}\")\n",
    "    print(\"Tip: Install xlsxwriter with 'pip install xlsxwriter' to enable Excel export\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60f9485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set a larger figure size\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Count discoveries by century (assuming Discovery_Year is a string like '1812')\n",
    "df['Century'] = df['Discovery_Year'].apply(\n",
    "    lambda x: '18th' if str(x).startswith('17') else\n",
    "              '19th' if str(x).startswith('18') else\n",
    "              '20th' if str(x).startswith('19') else\n",
    "              '21st' if str(x).startswith('20') else 'Unknown'\n",
    ")\n",
    "\n",
    "# Create a bar chart of discoveries by century\n",
    "century_counts = df['Century'].value_counts().sort_index()\n",
    "plt.bar(century_counts.index, century_counts.values, color='skyblue')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Century')\n",
    "plt.ylabel('Number of Discoveries')\n",
    "plt.title('Turtle Family Discoveries by Century')\n",
    "\n",
    "# Add value labels on top of each bar\n",
    "for i, v in enumerate(century_counts.values):\n",
    "    plt.text(i, v + 0.1, str(v), ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display percentage of discoveries by century\n",
    "century_percentage = (century_counts / century_counts.sum() * 100).round(1)\n",
    "print(\"\\nPercentage of Discoveries by Century:\")\n",
    "for century, percentage in century_percentage.items():\n",
    "    print(f\"{century}: {percentage}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4527e22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reusable function for future scraping projects\n",
    "def scrape_turtle_data(url, save_to_csv=True):\n",
    "    \"\"\"\n",
    "    A reusable function to scrape turtle family data from scrapethissite.com\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    url : str\n",
    "        The URL of the page containing turtle family cards\n",
    "    save_to_csv : bool, default=True\n",
    "        Whether to save the results to a CSV file\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        A DataFrame containing the scraped turtle data\n",
    "    \"\"\"\n",
    "    # Initialize session and get the page\n",
    "    session = requests.Session()\n",
    "    try:\n",
    "        response = session.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error accessing {url}: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Parse the HTML\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find all turtle cards\n",
    "    turtle_cards = soup.find_all('div', class_='col-md-4 turtle-family-card')\n",
    "    \n",
    "    if not turtle_cards:\n",
    "        print(\"No turtle cards found on the page\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Found {len(turtle_cards)} turtle families\")\n",
    "    \n",
    "    # Initialize data dictionary\n",
    "    data = {\n",
    "        \"Name\": [],\n",
    "        \"Known_As\": [],\n",
    "        \"Discovery_Year\": [],\n",
    "        \"Discovered_By\": []\n",
    "    }\n",
    "    \n",
    "    # Process each turtle card\n",
    "    for i, card in enumerate(turtle_cards):\n",
    "        print(f\"Processing turtle {i+1}/{len(turtle_cards)}...\")\n",
    "        \n",
    "        # Extract name\n",
    "        name_tag = card.find('h3', class_='family-name')\n",
    "        name = name_tag.text.strip() if name_tag else \"Unknown\"\n",
    "        data['Name'].append(name)\n",
    "        \n",
    "        # Find detail link\n",
    "        link_tag = card.find('a', href=True)\n",
    "        if link_tag:\n",
    "            # Get detail page\n",
    "            detail_url = 'https://www.scrapethissite.com' + link_tag['href']\n",
    "            try:\n",
    "                detail_response = session.get(detail_url, timeout=10)\n",
    "                detail_response.raise_for_status()\n",
    "                detail_soup = BeautifulSoup(detail_response.content, 'html.parser')\n",
    "                \n",
    "                # Extract information\n",
    "                info = detail_soup.find('p', class_='lead')\n",
    "                if info:\n",
    "                    # Get common name\n",
    "                    known_tag = info.find('strong', class_='common-name')\n",
    "                    known_as = known_tag.text.strip() if known_tag else \"Unknown\"\n",
    "                    \n",
    "                    # Get year\n",
    "                    year_match = re.search(r'\\b(1[7-9]\\d{2}|20\\d{2})\\b', info.text)\n",
    "                    year = year_match.group(0) if year_match else \"Unknown\"\n",
    "                    \n",
    "                    # Get discoverer\n",
    "                    discoverer_match = re.search(r'by ([A-Z][a-zA-Z\\s\\.\\-]*)', info.text)\n",
    "                    discoverer = discoverer_match.group(1).strip() if discoverer_match else \"Unknown\"\n",
    "                    \n",
    "                    # Add to data\n",
    "                    data['Known_As'].append(known_as)\n",
    "                    data['Discovery_Year'].append(year)\n",
    "                    data['Discovered_By'].append(discoverer)\n",
    "                else:\n",
    "                    data['Known_As'].append(\"Unknown\")\n",
    "                    data['Discovery_Year'].append(\"Unknown\")\n",
    "                    data['Discovered_By'].append(\"Unknown\")\n",
    "            except requests.exceptions.RequestException:\n",
    "                data['Known_As'].append(\"Error\")\n",
    "                data['Discovery_Year'].append(\"Error\")\n",
    "                data['Discovered_By'].append(\"Error\")\n",
    "        else:\n",
    "            data['Known_As'].append(\"No Link\")\n",
    "            data['Discovery_Year'].append(\"No Link\")\n",
    "            data['Discovered_By'].append(\"No Link\")\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Add ID column\n",
    "    df['ID'] = range(1, len(df) + 1)\n",
    "    df = df[['ID', 'Name', 'Known_As', 'Discovery_Year', 'Discovered_By']]\n",
    "    \n",
    "    # Save to CSV if requested\n",
    "    if save_to_csv:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f'turtle_data_{timestamp}.csv'\n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"Data saved to {filename}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "# new_df = scrape_turtle_data('https://www.scrapethissite.com/pages/frames/?frame=i')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b804979b",
   "metadata": {},
   "source": [
    "# Simple Web Scraping for iFrames\n",
    "\n",
    "This notebook demonstrates how to scrape content from a webpage that contains frames or iframes. We'll follow these steps:\n",
    "\n",
    "1. Load the parent page\n",
    "2. Find all frame/iframe elements\n",
    "3. Load the content of each frame\n",
    "4. Extract data from each frame\n",
    "5. Handle errors and avoid duplicates\n",
    "6. Organize the data into a DataFrame\n",
    "\n",
    "## What We're Scraping\n",
    "\n",
    "The example site \"scrapethissite.com\" has a frames demo page that we'll use to practice extracting content from multiple frames."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09acec1f",
   "metadata": {},
   "source": [
    "# Issues and Solutions\n",
    "\n",
    "When web scraping, you'll often encounter various challenges:\n",
    "\n",
    "## Common Issues Fixed in This Code:\n",
    "\n",
    "1. **404 Errors**: Some frames had invalid URLs that returned 404 errors\n",
    "   - *Solution*: Added proper error handling with try/except blocks\n",
    "\n",
    "2. **Duplicate Processing**: The same frame was being processed multiple times\n",
    "   - *Solution*: Added a set to track processed URLs\n",
    "\n",
    "3. **Missing Data**: Some expected elements were not found in the frames\n",
    "   - *Solution*: Added fallbacks to look for alternative elements and proper default values\n",
    "\n",
    "4. **Inconsistent Data Structure**: The DataFrame had mixed column types\n",
    "   - *Solution*: Standardized the data structure and filled missing values\n",
    "\n",
    "## Best Practices for Web Scraping:\n",
    "\n",
    "- Add delays between requests to avoid overloading the server\n",
    "- Handle errors gracefully\n",
    "- Check if content exists before trying to extract it\n",
    "- Avoid scraping too aggressively (which could get your IP blocked)\n",
    "- Respect robots.txt and website terms of service"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
