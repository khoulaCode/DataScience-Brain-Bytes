{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43ef519e-ba1c-4b68-b9b2-e57eb546beb4",
   "metadata": {},
   "source": [
    "# Neural Network Regression with California Housing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6666e788-6fea-47a7-b214-693286503125",
   "metadata": {},
   "source": [
    "### I will use a neural network to predict median house values using the California Housing dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f27facf-341c-4234-adb6-c64b66270ba0",
   "metadata": {},
   "source": [
    "### 1.Loading The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6fe0e4-1fbc-43e2-9326-27509f0b34c7",
   "metadata": {},
   "source": [
    "#### the dataset has 8 features and one target — median house value (MedHouseVal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1ac0910-7052-476b-a602-0440024b76b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude', 'MedHouseVal']\n",
      "count    20640.000000\n",
      "mean         2.068558\n",
      "std          1.153956\n",
      "min          0.149990\n",
      "25%          1.196000\n",
      "50%          1.797000\n",
      "75%          2.647250\n",
      "max          5.000010\n",
      "Name: MedHouseVal, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Load dataset \n",
    "data = fetch_california_housing(as_frame=True)\n",
    "df = data.frame\n",
    "\n",
    "# Target column is 'MedHouseVal'\n",
    "target_name = data.target_names[0]\n",
    "print(\"Columns:\", list(df.columns))\n",
    "print(df[target_name].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a84c64-15d0-45bb-b585-a778f5e58658",
   "metadata": {},
   "source": [
    "### 2.Split Data and Scale Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707e90c1-e900-4aad-b585-893fc880c4a9",
   "metadata": {},
   "source": [
    "#### Scaling helps neural networks train faster and improves stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92585c8a-d8db-4599-a720-d3ad0413c179",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop(columns=[target_name])\n",
    "y = df[target_name]\n",
    "\n",
    "# Train-validation split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_s = scaler.transform(X_train)\n",
    "X_val_s = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0aa5f1-bb23-4f92-a23a-7a46d35c96b0",
   "metadata": {},
   "source": [
    "### 3.Build A Regression Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b79a4fa-2731-4263-9a4a-f1cc9e02a266",
   "metadata": {},
   "source": [
    "#### ReLU activation and Adam optimizer are common for regression tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "035d545c-5390-48d1-a019-857f74916a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "\n",
    "n_features = X_train.shape[1]\n",
    "\n",
    "def build_model(hidden_units, activation='relu', optimizer='adam', lr=None):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Input(shape=(n_features,)))\n",
    "    for u in hidden_units:\n",
    "        model.add(layers.Dense(u, activation=activation))\n",
    "    # Output layer: linear activation for regression\n",
    "    model.add(layers.Dense(1, activation='linear'))\n",
    "\n",
    "    # Optimizer\n",
    "    if optimizer == 'adam':\n",
    "        opt = optimizers.Adam(learning_rate=lr or 0.001)\n",
    "    elif optimizer == 'sgd':\n",
    "        opt = optimizers.SGD(learning_rate=lr or 0.01, momentum=0.9)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported optimizer\")\n",
    "\n",
    "    model.compile(optimizer=opt, loss='mse', metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d076403-c7a2-4840-8412-a909c4d83b55",
   "metadata": {},
   "source": [
    "### 4.Train Models With Different Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56280f8e-1d55-4b6f-8bd6-3904c44d3b28",
   "metadata": {},
   "source": [
    "#### test different activations, optimizers, and depths to compare performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8130e32b-0cf6-4cfb-a67d-07ed688d7df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training relu_adam_shallow...\n",
      "\n",
      "Training tanh_adam_shallow...\n",
      "\n",
      "Training relu_sgd_shallow...\n",
      "\n",
      "Training relu_adam_deep...\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "configs = [\n",
    "    {'name':'relu_adam_shallow', 'hidden':[64,32], 'activation':'relu', 'optimizer':'adam'},\n",
    "    {'name':'tanh_adam_shallow', 'hidden':[64,32], 'activation':'tanh', 'optimizer':'adam'},\n",
    "    {'name':'relu_sgd_shallow',  'hidden':[64,32], 'activation':'relu', 'optimizer':'sgd'},\n",
    "    {'name':'relu_adam_deep',    'hidden':[128,64,32,16], 'activation':'relu', 'optimizer':'adam'},\n",
    "]\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "results = []\n",
    "histories = {}\n",
    "\n",
    "for cfg in configs:\n",
    "    print(f\"\\nTraining {cfg['name']}...\")\n",
    "    model = build_model(cfg['hidden'], activation=cfg['activation'], optimizer=cfg['optimizer'])\n",
    "    start = time.perf_counter()\n",
    "    history = model.fit(X_train_s, y_train,\n",
    "                        validation_data=(X_val_s, y_val),\n",
    "                        epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "    elapsed = time.perf_counter() - start\n",
    "\n",
    "    histories[cfg['name']] = history.history\n",
    "\n",
    "    # Save metrics\n",
    "    results.append({\n",
    "        'model': cfg['name'],\n",
    "        'val_mse': history.history['val_loss'][-1],\n",
    "        'val_mae': history.history['val_mae'][-1],\n",
    "        'time_s': elapsed\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8005d9e8-3e91-4284-90f5-f16f289a3f6b",
   "metadata": {},
   "source": [
    "### 5.Evaluate Models (MSE, MAE, RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae0d975-5dfb-4c36-bc33-d5674576c78c",
   "metadata": {},
   "source": [
    "#### ReLU + Adam (deep or shallow) performs best, while SGD is slower and less accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a714e45d-adcf-4f00-812f-2df3ed5d9e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m y_pred_train \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_train_s)\u001b[38;5;241m.\u001b[39mravel()\n\u001b[0;32m     11\u001b[0m y_pred_val \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_val_s)\u001b[38;5;241m.\u001b[39mravel()\n\u001b[1;32m---> 13\u001b[0m train_mse \u001b[38;5;241m=\u001b[39m mean_squared_error(y_train, y_pred_train)\n\u001b[0;32m     14\u001b[0m val_mse   \u001b[38;5;241m=\u001b[39m mean_squared_error(y_val, y_pred_val)\n\u001b[0;32m     15\u001b[0m train_mae \u001b[38;5;241m=\u001b[39m mean_absolute_error(y_train, y_pred_train)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    214\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    215\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    216\u001b[0m         )\n\u001b[0;32m    217\u001b[0m     ):\n\u001b[1;32m--> 218\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    224\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    226\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    227\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    228\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:580\u001b[0m, in \u001b[0;36mmean_squared_error\u001b[1;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Mean squared error regression loss.\u001b[39;00m\n\u001b[0;32m    531\u001b[0m \n\u001b[0;32m    532\u001b[0m \u001b[38;5;124;03mRead more in the :ref:`User Guide <mean_squared_error>`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    576\u001b[0m \u001b[38;5;124;03m0.825...\u001b[39;00m\n\u001b[0;32m    577\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    578\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(y_true, y_pred, sample_weight, multioutput)\n\u001b[0;32m    579\u001b[0m _, y_true, y_pred, sample_weight, multioutput \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 580\u001b[0m     _check_reg_targets_with_floating_dtype(\n\u001b[0;32m    581\u001b[0m         y_true, y_pred, sample_weight, multioutput, xp\u001b[38;5;241m=\u001b[39mxp\n\u001b[0;32m    582\u001b[0m     )\n\u001b[0;32m    583\u001b[0m )\n\u001b[0;32m    584\u001b[0m output_errors \u001b[38;5;241m=\u001b[39m _average((y_true \u001b[38;5;241m-\u001b[39m y_pred) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, weights\u001b[38;5;241m=\u001b[39msample_weight)\n\u001b[0;32m    586\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(multioutput, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:209\u001b[0m, in \u001b[0;36m_check_reg_targets_with_floating_dtype\u001b[1;34m(y_true, y_pred, sample_weight, multioutput, xp)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Ensures y_true, y_pred, and sample_weight correspond to same regression task.\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \n\u001b[0;32m    162\u001b[0m \u001b[38;5;124;03mExtends `_check_reg_targets` by automatically selecting a suitable floating-point\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;124;03m    correct keyword.\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    207\u001b[0m dtype_name \u001b[38;5;241m=\u001b[39m _find_matching_floating_dtype(y_true, y_pred, sample_weight, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[1;32m--> 209\u001b[0m y_type, y_true, y_pred, sample_weight, multioutput \u001b[38;5;241m=\u001b[39m _check_reg_targets(\n\u001b[0;32m    210\u001b[0m     y_true, y_pred, sample_weight, multioutput, dtype\u001b[38;5;241m=\u001b[39mdtype_name, xp\u001b[38;5;241m=\u001b[39mxp\n\u001b[0;32m    211\u001b[0m )\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y_type, y_true, y_pred, sample_weight, multioutput\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:116\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[1;34m(y_true, y_pred, sample_weight, multioutput, dtype, xp)\u001b[0m\n\u001b[0;32m    114\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    115\u001b[0m y_true \u001b[38;5;241m=\u001b[39m check_array(y_true, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m--> 116\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m check_array(y_pred, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    118\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(sample_weight, y_true, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1105\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1099\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1100\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m while dim <= 2 is required\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1102\u001b[0m     )\n\u001b[0;32m   1104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[1;32m-> 1105\u001b[0m     _assert_all_finite(\n\u001b[0;32m   1106\u001b[0m         array,\n\u001b[0;32m   1107\u001b[0m         input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m   1108\u001b[0m         estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[0;32m   1109\u001b[0m         allow_nan\u001b[38;5;241m=\u001b[39mensure_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1110\u001b[0m     )\n\u001b[0;32m   1112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[0;32m   1113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[0;32m   1114\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:120\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 120\u001b[0m _assert_all_finite_element_wise(\n\u001b[0;32m    121\u001b[0m     X,\n\u001b[0;32m    122\u001b[0m     xp\u001b[38;5;241m=\u001b[39mxp,\n\u001b[0;32m    123\u001b[0m     allow_nan\u001b[38;5;241m=\u001b[39mallow_nan,\n\u001b[0;32m    124\u001b[0m     msg_dtype\u001b[38;5;241m=\u001b[39mmsg_dtype,\n\u001b[0;32m    125\u001b[0m     estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[0;32m    126\u001b[0m     input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m    127\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:169\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    153\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    155\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    167\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    168\u001b[0m     )\n\u001b[1;32m--> 169\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN."
     ]
    }
   ],
   "source": [
    "import math\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import pandas as pd\n",
    "\n",
    "eval_results = []\n",
    "for cfg in configs:\n",
    "    model = build_model(cfg['hidden'], activation=cfg['activation'], optimizer=cfg['optimizer'])\n",
    "    model.fit(X_train_s, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "\n",
    "    y_pred_train = model.predict(X_train_s).ravel()\n",
    "    y_pred_val = model.predict(X_val_s).ravel()\n",
    "\n",
    "    train_mse = mean_squared_error(y_train, y_pred_train)\n",
    "    val_mse   = mean_squared_error(y_val, y_pred_val)\n",
    "    train_mae = mean_absolute_error(y_train, y_pred_train)\n",
    "    val_mae   = mean_absolute_error(y_val, y_pred_val)\n",
    "\n",
    "    eval_results.append({\n",
    "        'model': cfg['name'],\n",
    "        'train_mse': train_mse,\n",
    "        'val_mse': val_mse,\n",
    "        'train_mae': train_mae,\n",
    "        'val_mae': val_mae,\n",
    "        'train_rmse': math.sqrt(train_mse),\n",
    "        'val_rmse': math.sqrt(val_mse),\n",
    "    })\n",
    "\n",
    "df_eval = pd.DataFrame(eval_results)\n",
    "print(df_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f969da-6f15-400c-9d31-23e6fda31924",
   "metadata": {},
   "source": [
    "### 6.Plot Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdbf807-ed11-4831-a906-4e110ea1b9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for name, hist in histories.items():\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.plot(hist['loss'], label=\"train MSE\")\n",
    "    plt.plot(hist['val_loss'], label=\"val MSE\")\n",
    "    plt.title(name)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"MSE\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cc346e-536c-4e78-903e-0245c006029a",
   "metadata": {},
   "source": [
    "#### Smooth and converging curves indicate good learning; large gaps show overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349d98af-8d7a-45d8-a15e-b70697dc4a04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
