{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "672b37a7",
   "metadata": {},
   "source": [
    "# Neural-Network-Regression-with-California-Housing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "24521e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.utils import to_categorical\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154ae2a9",
   "metadata": {},
   "source": [
    "## What changes if I use relu vs tanh activations in hidden layers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec4e9da",
   "metadata": {},
   "source": [
    "First of all I will give you the big picture the Activations fanctions like ReLU and tanh: <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b9712a",
   "metadata": {},
   "source": [
    "**What are they exactly?** <br>\n",
    "<br>\n",
    "They are functions that work as checkpoints (neurons) and pass the signals (numerical outputs) between them. Each neuron first calculates a weighted sum of its inputs, and then the activation function transforms this value into an output. For example, if the raw signal is 1, the tanh function will squash it to about 0.76, while ReLU will let it pass through unchanged. In this way, activation functions control how much of the signal continues from one neuron to the next until the final output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4b87db",
   "metadata": {},
   "source": [
    "#### Answer the question:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5d72bd",
   "metadata": {},
   "source": [
    "The changes will be in the activation value of the neuron:<br>\n",
    "<br>\n",
    "**if i use ReLU:** the activation value of the neuron will be always positive, and the value unchange will be same for the raw signal. <br>\n",
    "<br>\n",
    "**if i use tanh:** the the activation value of the neuron will be squashed between the range of (-1 to 0) the smallest value will be near to -1 and the largest value will be near to 1, they are not come to exact 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5011ef08",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bec0a0a4",
   "metadata": {},
   "source": [
    "## Why should the output layer use a linear activation instead of sigmoid?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79542e20",
   "metadata": {},
   "source": [
    "Depend on the type of problems or requirenment we choose the type of the activation, and we have two types of activation: <br>\n",
    "<br>\n",
    "**1. Regression Activation.** <br>\n",
    "- We use this type of activation if we need to predict the continuous values which are larger than 1.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**2. Classification Activation.** <br>\n",
    "- We use this type of activation if our output is probability and category"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd3f315",
   "metadata": {},
   "source": [
    "#### Answer the question:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93aee7f",
   "metadata": {},
   "source": [
    "As I explained above, so if we want to get the prices which is continuous number so the best activation type  is \"Linear\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031465ac",
   "metadata": {},
   "source": [
    "## Do deeper models (more layers) always improve prediction accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4fdca5",
   "metadata": {},
   "source": [
    "The main point of saying it increases the accuracy is depend on the how complex the data set is, and increasing the hidden layers can be both negative and positive effects: <br>\n",
    "<br>\n",
    "If we increase the hidded layers in the complex data that will lead to the better accuracy. Why? because it makes the model learn more patterns within the dataset, and vise versa, if we increase the hidden layers in the dataset which have few rows that will lead to the overfitting rather than learning from the patterns and this willl made the model to memorize and then in testing phase the accuracy will be lower than training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3802275",
   "metadata": {},
   "source": [
    "## How do different metrics (MSE, MAE, RMSE) affect my interpretation of results?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0342e53b",
   "metadata": {},
   "source": [
    "MSE and RMSE highlight big mistakes (RMSE is just easier to read since it’s in the same units as the target), while MAE gives the average error size and is more forgiving of outliers. In practice, MAE tells you the “typical miss,” and RMSE shows how bad the bigger misses can get."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fa1985",
   "metadata": {},
   "source": [
    "## Coding Time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2575581f",
   "metadata": {},
   "source": [
    "#### Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "90b43089",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_california_housing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "be690bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data.data, data.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12371a8",
   "metadata": {},
   "source": [
    "#### Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "cf7de59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test  = to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5860bc9",
   "metadata": {},
   "source": [
    "- Good overlap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0aa7f3",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "55f008b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler() # The job of it it keep the mean = 0 and the Std = 1\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8ce63b",
   "metadata": {},
   "source": [
    "### Build a regression model with at least two hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "1ebc0396",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python313\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Flatten(input_shape=(X_train.shape[1],)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "3efb1d90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_17\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_17\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_51 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_52 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten_14 (\u001b[38;5;33mFlatten\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_50 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m576\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_51 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_52 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,689</span> (10.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,689\u001b[0m (10.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,689</span> (10.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,689\u001b[0m (10.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cf4960",
   "metadata": {},
   "source": [
    "### Try different optimizers (adam, sgd) and compare training time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590ce742",
   "metadata": {},
   "source": [
    "##### An optimizer is the method the neural network uses to update its weights during training so that it gets better at predicting. <br>\n",
    "1. First it assign the random weights in the learnable links. <br>\n",
    "2. The optimizer adjusts these weights step by step to minimize the loss function (the error between predictions and actual values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "e5da87fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0900 - mae: 0.1801 - val_loss: 0.0900 - val_mae: 0.1800\n",
      "Epoch 2/5\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0900 - mae: 0.1800 - val_loss: 0.0900 - val_mae: 0.1800\n",
      "Epoch 3/5\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0900 - mae: 0.1800 - val_loss: 0.0900 - val_mae: 0.1800\n",
      "Epoch 4/5\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0900 - mae: 0.1800 - val_loss: 0.0900 - val_mae: 0.1799\n",
      "Epoch 5/5\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0900 - mae: 0.1800 - val_loss: 0.0900 - val_mae: 0.1804\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer= \"adam\" , loss=\"mse\", metrics=[\"mae\"])\n",
    "history = model.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "1e97a60c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m2972/2972\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 834us/step - loss: 0.2975 - mae: 0.2117 - val_loss: 0.1074 - val_mae: 0.1939\n",
      "Epoch 2/5\n",
      "\u001b[1m2972/2972\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 849us/step - loss: 0.0916 - mae: 0.1801 - val_loss: 0.0902 - val_mae: 0.1789\n",
      "Epoch 3/5\n",
      "\u001b[1m2972/2972\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 796us/step - loss: 0.0902 - mae: 0.1800 - val_loss: 0.0901 - val_mae: 0.1804\n",
      "Epoch 4/5\n",
      "\u001b[1m2972/2972\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 867us/step - loss: 0.0901 - mae: 0.1800 - val_loss: 0.0901 - val_mae: 0.1797\n",
      "Epoch 5/5\n",
      "\u001b[1m2972/2972\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 880us/step - loss: 0.0901 - mae: 0.1800 - val_loss: 0.0901 - val_mae: 0.1806\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer= \"sgd\" , loss=\"mse\", metrics=[\"mae\"])\n",
    "history = model.fit(X_train, y_train, epochs=5, batch_size=5, validation_split=0.1, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c7ab49",
   "metadata": {},
   "source": [
    "### Report metrics like MSE and MAE on training vs validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "48e5d905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training MSE: 0.0901\n",
      "Final Training MAE: 0.1800\n",
      "Final Validation MSE: 0.0901\n",
      "Final Validation MAE: 0.1806\n"
     ]
    }
   ],
   "source": [
    "hist_df = pd.DataFrame(history.history)\n",
    "\n",
    "# Training metrics\n",
    "train_mse = hist_df[\"loss\"]      # because \"loss\" = MSE here\n",
    "train_mae = hist_df[\"mae\"]\n",
    "\n",
    "# Validation metrics\n",
    "val_mse = hist_df[\"val_loss\"]    # val_loss = validation MSE\n",
    "val_mae = hist_df[\"val_mae\"]\n",
    "\n",
    "# Report last epoch values\n",
    "print(f\"Final Training MSE: {train_mse.iloc[-1]:.4f}\")\n",
    "print(f\"Final Training MAE: {train_mae.iloc[-1]:.4f}\")\n",
    "print(f\"Final Validation MSE: {val_mse.iloc[-1]:.4f}\")\n",
    "print(f\"Final Validation MAE: {val_mae.iloc[-1]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
