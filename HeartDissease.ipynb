{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36f4ce8a",
   "metadata": {},
   "source": [
    "# Feature Scaling & Normalization with Heart Disease"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da25621",
   "metadata": {},
   "source": [
    "---\n",
    " ## the goal of this task\n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb8bf39",
   "metadata": {},
   "source": [
    "##### I want to build a classification model on the Heart Disease dataset (predicting whether a patient has heart disease), so that I can learn how feature scaling and normalization affect neural network training and convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d564303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find kaggle.json. Please download your API token from https://www.kaggle.com/settings/account and place it in C:\\Users\\bbuser\\.kaggle\\kaggle.json\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce762fb",
   "metadata": {},
   "source": [
    "---\n",
    "### load data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d782d44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bbuser\\Desktop\\DataSience-brain-bytes\\DataScience-Brain-Bytes\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "Raw Data Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.75      0.75       102\n",
      "           1       0.75      0.75      0.75       103\n",
      "\n",
      "    accuracy                           0.75       205\n",
      "   macro avg       0.75      0.75      0.75       205\n",
      "weighted avg       0.75      0.75      0.75       205\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load dataset (UCI Heart Disease dataset)\n",
    "url = \"C:\\\\Users\\\\bbuser\\\\Downloads\\\\heart.csv\"\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "# Features and target\n",
    "X = data.drop(\"target\", axis=1)\n",
    "y = data[\"target\"]\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "def build_model():\n",
    "    model = Sequential([\n",
    "        Dense(16, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        Dense(8, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Raw data model\n",
    "model_raw = build_model()\n",
    "history_raw = model_raw.fit(X_train, y_train, validation_split=0.2, epochs=50, verbose=0)\n",
    "\n",
    "# Evaluation\n",
    "y_pred_raw = (model_raw.predict(X_test) > 0.5).astype(int)\n",
    "print(\"Raw Data Performance:\")\n",
    "print(classification_report(y_test, y_pred_raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8683b94e",
   "metadata": {},
   "source": [
    "## Q2: Which scaling method works better: MinMaxScaler vs StandardScaler?\n",
    "\n",
    "### Answer:\n",
    "- **MinMaxScaler (Normalization):** Scales all features into [0,1].  \n",
    "  Works well when input features don’t follow a Gaussian distribution.  \n",
    "- **StandardScaler (Standardization):** Transforms features to have mean=0 and std=1.  \n",
    "  Works better when features have different variances and distributions close to normal.  \n",
    "\n",
    "We’ll compare both:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5d7d479",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bbuser\\Desktop\\DataSience-brain-bytes\\DataScience-Brain-Bytes\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "MinMaxScaler Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.72      0.78       102\n",
      "           1       0.76      0.88      0.82       103\n",
      "\n",
      "    accuracy                           0.80       205\n",
      "   macro avg       0.81      0.80      0.80       205\n",
      "weighted avg       0.81      0.80      0.80       205\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bbuser\\Desktop\\DataSience-brain-bytes\\DataScience-Brain-Bytes\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 15 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000016C04A79C60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "StandardScaler Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.78      0.82       102\n",
      "           1       0.81      0.88      0.84       103\n",
      "\n",
      "    accuracy                           0.83       205\n",
      "   macro avg       0.84      0.83      0.83       205\n",
      "weighted avg       0.84      0.83      0.83       205\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# MinMax Scaled data\n",
    "scaler_minmax = MinMaxScaler()\n",
    "X_train_minmax = scaler_minmax.fit_transform(X_train)\n",
    "X_test_minmax = scaler_minmax.transform(X_test)\n",
    "\n",
    "model_minmax = build_model()\n",
    "history_minmax = model_minmax.fit(X_train_minmax, y_train, validation_split=0.2, epochs=50, verbose=0)\n",
    "\n",
    "y_pred_minmax = (model_minmax.predict(X_test_minmax) > 0.5).astype(int)\n",
    "print(\"MinMaxScaler Performance:\")\n",
    "print(classification_report(y_test, y_pred_minmax))\n",
    "\n",
    "\n",
    "# Standard Scaled data\n",
    "scaler_standard = StandardScaler()\n",
    "X_train_std = scaler_standard.fit_transform(X_train)\n",
    "X_test_std = scaler_standard.transform(X_test)\n",
    "\n",
    "model_std = build_model()\n",
    "history_std = model_std.fit(X_train_std, y_train, validation_split=0.2, epochs=50, verbose=0)\n",
    "\n",
    "y_pred_std = (model_std.predict(X_test_std) > 0.5).astype(int)\n",
    "print(\"StandardScaler Performance:\")\n",
    "print(classification_report(y_test, y_pred_std))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4890a3b",
   "metadata": {},
   "source": [
    "## Q3: Do categorical features need to be one-hot encoded, and how does that affect performance?\n",
    "\n",
    "### Answer:\n",
    "Yes. Neural networks expect numeric inputs. If categorical variables are present, they must be **one-hot encoded**.  \n",
    "In the Heart Disease dataset, features like `sex`, `cp`, `thal`, and `slope` are categorical. Encoding prevents the model from assuming an ordinal relationship between categories.\n",
    "\n",
    "We’ll preprocess categorical variables with **OneHotEncoder** and compare performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bcf6f18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bbuser\\Desktop\\DataSience-brain-bytes\\DataScience-Brain-Bytes\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 15 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000016C05FF9D00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "With One-Hot Encoding Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.77      0.84       102\n",
      "           1       0.81      0.94      0.87       103\n",
      "\n",
      "    accuracy                           0.86       205\n",
      "   macro avg       0.87      0.86      0.86       205\n",
      "weighted avg       0.87      0.86      0.86       205\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "categorical = ['sex','cp','fbs','restecg','exang','slope','thal']\n",
    "numeric = [col for col in X.columns if col not in categorical]\n",
    "\n",
    "# Column transformer: OneHotEncode categorical, scale numeric\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', StandardScaler(), numeric),\n",
    "    ('cat', OneHotEncoder(drop='first'), categorical)])\n",
    "\n",
    "X_train_enc = preprocessor.fit_transform(X_train)\n",
    "X_test_enc = preprocessor.transform(X_test)\n",
    "\n",
    "# Build a model with input shape matching the encoded features\n",
    "def build_model_enc():\n",
    "    model = Sequential([\n",
    "        Dense(16, activation='relu', input_shape=(X_train_enc.shape[1],)),\n",
    "        Dense(8, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model_enc = build_model_enc()\n",
    "history_enc = model_enc.fit(X_train_enc, y_train, validation_split=0.2, epochs=50, verbose=0)\n",
    "\n",
    "y_pred_enc = (model_enc.predict(X_test_enc) > 0.5).astype(int)\n",
    "print(\"With One-Hot Encoding Performance:\")\n",
    "print(classification_report(y_test, y_pred_enc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20106626",
   "metadata": {},
   "source": [
    "## Q4: How sensitive is the neural network to changes in learning rate when features are scaled vs unscaled?\n",
    "\n",
    "### Answer:\n",
    "- On **unscaled data**, large feature values cause unstable gradients → learning rate must be very small (0.0001) to avoid divergence.  \n",
    "- On **scaled data**, learning rate can be larger (0.001–0.01), leading to faster convergence.  \n",
    "\n",
    "We’ll compare training with different learning rates:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e8facf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bbuser\\Desktop\\DataSience-brain-bytes\\DataScience-Brain-Bytes\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "c:\\Users\\bbuser\\Desktop\\DataSience-brain-bytes\\DataScience-Brain-Bytes\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR=0.0001 | Unscaled Acc=0.551 | Scaled Acc=0.732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bbuser\\Desktop\\DataSience-brain-bytes\\DataScience-Brain-Bytes\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR=0.001 | Unscaled Acc=0.771 | Scaled Acc=0.800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bbuser\\Desktop\\DataSience-brain-bytes\\DataScience-Brain-Bytes\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR=0.01 | Unscaled Acc=0.776 | Scaled Acc=0.951\n"
     ]
    }
   ],
   "source": [
    "def train_with_lr(lr, scaled=True):\n",
    "    if scaled:\n",
    "        X_tr, X_te = X_train_std, X_test_std\n",
    "    else:\n",
    "        X_tr, X_te = X_train, X_test\n",
    "\n",
    "    model = Sequential([\n",
    "        Dense(16, activation='relu', input_shape=(X_tr.shape[1],)),\n",
    "        Dense(8, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    history = model.fit(X_tr, y_train, validation_split=0.2, epochs=50, verbose=0)\n",
    "    results = model.evaluate(X_te, y_test, verbose=0)\n",
    "    return results\n",
    "\n",
    "for lr in [0.0001, 0.001, 0.01]:\n",
    "    res_unscaled = train_with_lr(lr, scaled=False)\n",
    "    res_scaled = train_with_lr(lr, scaled=True)\n",
    "    print(f\"LR={lr} | Unscaled Acc={res_unscaled[1]:.3f} | Scaled Acc={res_scaled[1]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0c529f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5bef681",
   "metadata": {},
   "source": [
    "#  Final Observations\n",
    "\n",
    "1. **Raw (Unscaled Data):**\n",
    "   - Training converges very slowly.\n",
    "   - Accuracy is lower compared to scaled versions.\n",
    "\n",
    "2. **MinMaxScaler vs StandardScaler:**\n",
    "   - Both improve convergence and accuracy.\n",
    "   - StandardScaler usually works better for this dataset (because distributions are closer to normal).\n",
    "\n",
    "3. **One-Hot Encoding:**\n",
    "   - Boosts performance since categorical variables are properly represented.\n",
    "   - Prevents false ordinal relationships.\n",
    "\n",
    "4. **Learning Rate Sensitivity:**\n",
    "   - Unscaled data requires very small learning rates to avoid divergence.\n",
    "   - Scaled data allows faster convergence with higher learning rates.\n",
    "\n",
    " **Conclusion:** Scaling and encoding significantly improve neural network training and predictive performance.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
